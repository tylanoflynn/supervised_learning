{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, FunctionTransformer, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"all_data.csv\")\n",
    "\n",
    "y = df[\"sold_price\"]\n",
    "X = df.drop(\"sold_price\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "cat_feats = df.dtypes[df.dtypes == 'object'].index.tolist()\n",
    "num_feats = df.dtypes[~df.dtypes.index.isin(cat_feats)].index.tolist()\n",
    "list_feats = []\n",
    "for category in cat_feats:\n",
    "    if type(df.loc[0, category]) == list:\n",
    "        cat_feats.remove(category)\n",
    "        list_feats.append(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numFeat(data):\n",
    "    return data[num_feats]\n",
    "\n",
    "def catFeat(data):\n",
    "    return data[cat_feats]\n",
    "\n",
    "def listFeat(data):\n",
    "    return data[list_feats]\n",
    "\n",
    "keep_num = FunctionTransformer(numFeat)\n",
    "keep_cat = FunctionTransformer(catFeat)\n",
    "keep_list = FunctionTransformer(listFeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "class_encoder = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potential models\n",
    "base_model = Ridge()\n",
    "rf_model = RandomForestRegressor()\n",
    "gb_model = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipe = Pipeline([\n",
    "    ('NumFilter', keep_num),\n",
    "    ('MMScaler', minmax_scaler),\n",
    "    ('Imputer', imputer)\n",
    "    \n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    ('CatFilter', keep_cat),\n",
    "    ('OHEncode', class_encoder),\n",
    "    ('Imputer', imputer)\n",
    "])\n",
    "\n",
    "list_pipe = Pipeline([\n",
    "    ('ListFilter', keep_list),\n",
    "    ('MultiLabelBinarizer', mlb),\n",
    "    ('Imputer', imputer)\n",
    "])\n",
    "\n",
    "pre_processing_pipeline = FeatureUnion([(\"num\", num_pipe), (\"cat\", cat_pipe), (\"list\", list_pipe)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([(\"preprocessing\", pre_processing_pipeline), (\"model\", None)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"model\": [base_model, rf_model, gb_model]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, params, verbose=10, refit=True)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make sure that we save our models.  In the old days, one just simply pickled (serialized) the model.  Now, however, certain model types have their own save format.  If the model is from sklearn, it can be pickled, if it's xgboost, for example, the newest format to save it in is JSON, but it can also be pickled.  It's a good idea to stay with the most current methods. \n",
    "- you may want to create a new `models/` subdirectory in your repo to stay organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your best model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've identified which model works the best, implement a prediction pipeline to make sure that you haven't leaked any data, and that the model could be easily deployed if desired.\n",
    "- Your pipeline should load the data, process it, load your saved tuned model, and output a set of predictions\n",
    "- Assume that the new data is in the same JSON format as your original data - you can use your original data to check that the pipeline works correctly\n",
    "- Beware that a pipeline can only handle functions with fit and transform methods.\n",
    "- Classes can be used to get around this, but now sklearn has a wrapper for user defined functions.\n",
    "- You can develop your functions or classes in the notebook here, but once they are working, you should import them from `functions_variables.py` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines come from sklearn.  When a pipeline is pickled, all of the information in the pipeline is stored with it.  For example, if we were deploying a model, and we had fit a scaler on the training data, we would want the same, already fitted scaling object to transform the new data with.  This is all stored when the pipeline is pickled.\n",
    "- save your final pipeline in your `models/` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your pipeline here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
